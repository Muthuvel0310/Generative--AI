{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf1d768",
   "metadata": {},
   "source": [
    "# Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67787689",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow matplotlib numpy pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba2014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8747a30",
   "metadata": {},
   "source": [
    "# Loading the Dataset (Sample Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34826d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path, img_size=(64, 64)):\n",
    "    images = []\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            img_path = os.path.join(dataset_path, filename)\n",
    "            img = Image.open(img_path)\n",
    "            img = img.resize(img_size)\n",
    "            img = np.array(img)\n",
    "            images.append(img)\n",
    "    images = np.array(images)\n",
    "    images = images.astype(np.float32) / 127.5 - 1  # Normalize images to [-1,1]\n",
    "    return images\n",
    "\n",
    "dataset_path = 'path/to/your/dataset'  # Provide the path to your dataset\n",
    "images = load_dataset(dataset_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0a5564",
   "metadata": {},
   "source": [
    "# Model Preparation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3b6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "def build_generator():\n",
    "    model = Sequential([\n",
    "        layers.Dense(128, input_shape=(100,)),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dense(256),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dense(64 * 64 * 3, activation='tanh'),  # Output shape: (64, 64, 3)\n",
    "        layers.Reshape((64, 64, 3))\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Discriminator\n",
    "def build_discriminator():\n",
    "    model = Sequential([\n",
    "        layers.Flatten(input_shape=(64, 64, 3)),\n",
    "        layers.Dense(256),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dense(128),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Build GAN\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    discriminator.trainable = False  # Freeze the discriminator for GAN training\n",
    "\n",
    "    gan = Sequential([\n",
    "        generator,\n",
    "        discriminator\n",
    "    ])\n",
    "    gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return gan\n",
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "gan = build_gan(generator, discriminator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e021c643",
   "metadata": {},
   "source": [
    "# Training the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f06f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, gan, images, epochs=1000, batch_size=32):\n",
    "    num_batches = images.shape[0] // batch_size\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(num_batches):\n",
    "            # Sample a random batch of real images\n",
    "            idx = np.random.randint(0, images.shape[0], batch_size)\n",
    "            real_images = images[idx]\n",
    "\n",
    "            # Generate a batch of fake images\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "            fake_images = generator.predict(noise)\n",
    "\n",
    "            # Train discriminator on real and fake images\n",
    "            d_loss_real = discriminator.train_on_batch(real_images, np.ones((batch_size, 1)))\n",
    "            d_loss_fake = discriminator.train_on_batch(fake_images, np.zeros((batch_size, 1)))\n",
    "\n",
    "            # Train generator\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "            g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "        # Output loss and save generated images periodically\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch: {epoch}, D Loss: {d_loss_real[0] + d_loss_fake[0]}, G Loss: {g_loss}\")\n",
    "            save_generated_images(generator, epoch)\n",
    "            \n",
    "def save_generated_images(generator, epoch, num_images=5):\n",
    "    noise = np.random.normal(0, 1, (num_images, 100))\n",
    "    generated_images = generator.predict(noise)\n",
    "\n",
    "    # Visualize and save generated images\n",
    "    for i in range(num_images):\n",
    "        plt.imshow((generated_images[i] + 1) / 2)  # Convert back from [-1,1] to [0,1]\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f'generated_images/epoch_{epoch}_image_{i}.png')\n",
    "        plt.show()\n",
    "\n",
    "train_gan(generator, discriminator, gan, images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179693ed",
   "metadata": {},
   "source": [
    "# Generating and Visualizing New Art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b9cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_visualize_new_art(generator, num_images=5):\n",
    "    noise = np.random.normal(0, 1, (num_images, 100))\n",
    "    generated_images = generator.predict(noise)\n",
    "\n",
    "    # Visualize generated images\n",
    "    for i in range(num_images):\n",
    "        plt.imshow((generated_images[i] + 1) / 2)  # Convert back from [-1,1] to [0,1]\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "generate_and_visualize_new_art(generator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cae5be",
   "metadata": {},
   "source": [
    "# Saving and Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27128ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "generator.save('generator_model.h5')\n",
    "discriminator.save('discriminator_model.h5')\n",
    "\n",
    "# Load the models\n",
    "loaded_generator = tf.keras.models.load_model('generator_model.h5')\n",
    "loaded_discriminator = tf.keras.models.load_model('discriminator_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74a2d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
